# FINDMINE Scrift Process: Store 10

Scrift stands for the combination of scraping and sifting feeds that our clients provide us with their site updates. 

All the necessary files can be accessed at Mishki's scrift repository on [Github][git].

## Step 0: Setup

Log into the dev1 server and create a new tmux session called scrift

```sh
$ tmux new -s scrift
```
Then get the latest changes from the [scrift repo][git].

```sh
$ cd ~/scrift
$ git fetch
$ git merge origin/master
```

## Step 1: Fetch
The first step in this process is to run ```fetch.sh```.
 This script copies all the files from the sftp directory to your own personal feed folder.
##### Before Running:
Depending on your file system configuration, you may need to adjust the path specified in fetch. You can alternatively add your own personal configuration to the script.
##### Run fetch:
Now, run the fetch script.
```sh
$ ./fetch.sh
```
##### After Running:
1. Validate that the files were fetched and copied correctly by comparing the timestamp and file size with the originals in the sftp directory
2. Commit the files from sftp to github.
```sh
username@sftp1:/home/store/feed$ git commit feed_file_timestamp.xml origin master
```
3. Delete the files from sftp. **NOTE: You need to be careful that new files are not dropping when you are deleting the files. If new files get dropped, just rerun fetch.**
```sh
username@sftp1:/home/store/feed$ rm *.csv
```

## Step 2: To JSON

To JSON takes the feed items and converts it to a single JSON file which is called ```latest.json```

For Store 10 specifically, you **need** to run ```to_json.py``` with the available argument specified **daily**.

```sh
$ ./to_json.py --available
```

****
The available argument, as well as other optional arguments are described below:
### Available:
The available argument goes through the entire feed directory, and visits the url of each item to create a unique set of items that are live on the store's site. It returns a smaller set of JSON objects of just the live items.

######Note: ```to_json.py``` does not update the database, it just returns subset of only 'live' / 'in stock' items

#### Path:
The path argument option allows you to specify a specific path to the feed directory if you do not want to use the default configuration. **The default configuration, which happens if no path argument is supplied, is to select the ```feed``` directory**.
```
$ ./to_json.py --path /some/specific/path
```

```
$ ./to_json.py -p /some/specific/path
```
#### Print:
The print argument, if specified, prints the JSON objects to the screen instead of writing them to a file.
```
$ ./to_json.py --print 
```

## Step 3: To DSL
```to_dsl.py``` takes the feed files and converts them to  .dsl file (by first calling ```./to_json.py``` without the ```--available``` argument), in order to put them in the right format for pushing to our elastic search.

**Note: Running to_dsl.py will overwrite the file ```latest.json``` that may have been created by running ```./to_json.py -- available ```. It's recommended that you rename the ```latest.json``` file generated by ```./to_json.py --available``` to ```available.json``` before running ```./to_dsl.py```**

To run to_dsl.py, simply type:
```sh
$ ./to_dsl.py
```

When ```to_dsl.py``` has completed execution, you will have a new file called ```latest.dsl``` in your working directory.
*****

As with to_json.py, there are various additional arguments that you can specify if desired. These are described below:

#### Path:
The path argument option allows you to specify a specific path to the feed directory if you do not want to use the default configuration. **The default configuration, which happens if no path argument is supplied, is to select the ```feed``` directory**.
```
$ ./to_json.py --path /some/specific/path
```

```
$ ./to_json.py -p /some/specific/path
```
#### Print:
The print argument, if specified, prints the JSON objects to the screen instead of writing them to a file.
```
$ ./to_json.py --print 
```

## Step 4: Push Feed to Elastic Search Script
Next, you need to run the ```push_elastic.sh``` script which, when run without arguments, takes the ```latest.dsl``` file created from the feed and pushes it to elastic search on the **dev server**. 

In most cases, you will want to push to the elastic search of both the dev and the production servers. This means running the ```push_elastic.sh``` script like so:

```sh
$ ./push_elastic.sh
```
which will push to the dev server, followed by

```sh
$ ./push_elastic.sh -h 127.0.0.1
```
replacing ```localhost 127.0.0.1``` with the production server IP address.
********

```to_elastic.sh``` has the following optional arguments:

#### Filename
If you want to specify a filename that is different than the default ```latest.dsl``` file, you can do so by running
```sh
$ ./push_elastic.sh --file filename.dsl
```
or 
```sh
$ ./push_elastic.sh -f filename.dsl
```

#### Host
The default configuration pushes the ```latest.dsl``` file to the dev server. However, if you want to push to the production server, you should specify the production server host address.

```sh
$ ./push_elastic.sh --host 127.0.0.1
```
or 
```sh
$ ./push_elastic.sh -h 127.0.0.1
```

####Port
The default configuration pushes the ```latest.dsl``` file to the dev server, which is listening for elastic search on ```PORT 9200```. However, if you want to a different port number, you can do so using the port argument.

```sh
$ ./push_elastic.sh --port 8888
```
or 
```sh
$ ./push_elastic.sh -p 8888
```

## Step 5: Update the cache in Redis and Elastic Search

**NOTE: If you are completing the scrift process for multiple stores, you only need to run Step 5 once after completing Steps 1 - 4 for each store.**

### Step 5.1 Fetch Redis
The ```fetch_redis.py``` program creates a ```latest.redis``` file in the current working directory. 

First, enter the ```cache``` directory:
```sh
$ cd ../cache
```
Then run the ```fetch_redis.py``` program:
```sh
$ ./fetch_redis.py
```
### Step 5.2 Push Redis
The ```push_redis.sh``` script pushes ```latest.redis``` to the dev server cache by default. In most situations, you will want to run ```push_redis.sh``` pointing both at dev and at the production server.

Do this by running:
```sh
$ ./push_redis.sh
```
```sh
$ ./push_redis.sh -h 127.0.0.1
```
Change ```127.0.0.1``` to the correct production server IP address.
*******

As with ```push_elastic.sh```, ```push_redis.sh``` has the following optional arguments:

#### Filename
If you want to specify a filename that is different than the default ```latest.redis``` file, you can do so by running
```sh
$ ./push_redis.sh --file filename.redis
```
or 
```sh
$ ./push_redis.sh -f filename.redis
```

#### Host
The default configuration pushes the ```latest.redis``` file to the dev server. However, if you want to push to the production server, you should specify the production server host address.

```sh
$ ./push_redis.sh --host 127.0.0.1
```
or 
```sh
$ ./push_redis.sh -h 127.0.0.1
```

####Port
The default configuration pushes the ```latest.redis``` file to the dev server, which is listening for redis on ```PORT 7001```. However, if you want to a different port number, you can do so using the port argument.

```sh
$ ./push_redis.sh --port 8888
```
or 
```sh
$ ./push_redis.sh -p 8888
```


### Step 5.3 Fetch Elastic
The ```fetch_elastic.py``` in the ```cache``` directory selects  the ```item_id```, ```title```, ```item_url```, ```product_id``` and ```description``` for every item in the mysql database on the **production** server and creates a file ```latest.dsl``` with JSON objects for all the items.

To run, simply type the following command in the ```cache``` directory:
```sh
$ ./fetch_elastic.py
```

### Step 5.4 Push Elastic

The ```push_elastic.sh``` script pushes ```latest.dsl``` made up of all items from the **production mysql database**to the dev server elastic search by default. In most situations, you will want to run ```push_elastic.sh``` pointing both at dev and at the production server.

Do this by running:
```sh
$ ./push_elastic.sh
```
```sh
$ ./push_elastic.sh -h 127.0.0.1
```
Change ```127.0.0.1``` to the correct production server IP address.
***************

 ```push_elastic.sh``` has the following optional arguments:

#### Filename
If you want to specify a filename that is different than the default ```latest.dsl``` file, you can do so by running
```sh
$ ./push_elastic.sh --file filename.dsl
```
or 
```sh
$ ./push_elastic.sh -f filename.dsl
```

#### Host
The default configuration pushes the ```latest.dsl``` file to the dev server. However, if you want to push to the production server, you should specify the production server host address.

```sh
$ ./push_elastic.sh --host 127.0.0.1
```
or 
```sh
$ ./push_elastic.sh -h 127.0.0.1
```

####Port
The default configuration pushes the ```latest.dsl``` file to the dev server, which is listening for elastic search on ```PORT 9200```. However, if you want to a different port number, you can do so using the port argument.

```sh
$ ./push_elastic.sh --port 8888
```
or 
```sh
$ ./push_elastic.sh -p 8888
```

## Conclusion
You have now successfully performed the scrift process for Store 10. If you have any issues with this documentation, email Angela at angela.fox@findmine.com.










[git]: <https://github.com/Mishki/scrift>












